{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test)=keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de la fonction appelée par le tuner d'hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\",min_value=0,max_value=8,default=0)\n",
    "    learning_rate = hp.Float(\"learning_rate\",min_value=1e-4,max_value=1e-2,sampling=\"log\") # par défaut, default = min_value # sampling log pour assurer une répartission égale entre 1e-4 et 1e-2\n",
    "    n_neurons = hp.Int(\"n_neurons\",min_value = 16,max_value = 256)\n",
    "    optimizers = hp.Choice(\"optimizer\",values = [\"adam\",\"sgd\"])\n",
    "    if optimizers == \"sgd\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons,activation=\"relu\"))\n",
    "    \n",
    "    model.add(keras.layers.Dense(10,activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un tuner qui cherche des combinaisons random d'hyperparamètres (qui viennent avant le fit du modele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tuner = kt.RandomSearch(build_model, #va chercher parmi les models spécifié par cette fn\n",
    "                               objective=\"val_accuracy\", # va classer en fonction de l'accuracy du jeu de validation\n",
    "                               max_trials=4, # ne va pas faire plus de 4 tirages parmi tous les choix de models possible\n",
    "                               overwrite=True, # Réinitialise les données à chaque fois qu'on appelle seach()\n",
    "                               directory=\"my_fashion_mnist\", # données rangées dans le dossier my_fashion_mnist\n",
    "                               project_name=\"my_rnd_research\", # dans un sous-répertoire my_rns_research\n",
    "                               seed=42)\n",
    "\n",
    "# ATTENTION!!!\n",
    "# Il vaut mieux en général mettre overwrite = False comme ça, on garde les données précédente et cela permet de comparer les précédents modèles\n",
    "# aux nouveaux quand on va reappeler la fonction search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Complete [00h 00m 39s]\n",
      "val_accuracy: 0.6957499980926514\n",
      "\n",
      "Best val_accuracy So Far: 0.6957499980926514\n",
      "Total elapsed time: 00h 02m 12s\n"
     ]
    }
   ],
   "source": [
    "random_tuner.search(X_train,y_train,epochs = 8, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoir le meilleur model parmi les 3 premiers\n",
    "top3_models = random_tuner.get_best_models(num_models=3)\n",
    "best_model = top3_models[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_hidden': 8,\n",
       " 'learning_rate': 0.00015159319577885927,\n",
       " 'n_neurons': 248,\n",
       " 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoir les meilleurs params parmi les parmas des 3 models arrivés premiers\n",
    "top3_params = random_tuner.get_best_hyperparameters(num_trials=3)\n",
    "top3_params[0].values #top3_params[0] est un objet et on appelle ses valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 8\n",
      "learning_rate: 0.00015159319577885927\n",
      "n_neurons: 248\n",
      "optimizer: adam\n",
      "Score: 0.6957499980926514\n"
     ]
    }
   ],
   "source": [
    "#Retourner les spécificités du meilleur model:\n",
    "best_trial = random_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_trial.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Régler les parmètres de fit du model (taille du lot, de quel manière preprocess les data ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationHypermodel(kt.HyperModel):\n",
    "\n",
    "    def build(self, hp):\n",
    "        return build_model(hp)\n",
    "    \n",
    "    def fit(self, hp, model, X,y, **kwargs):\n",
    "        if hp.Boolean(\"normilize\"):\n",
    "            X = keras.layers.Normalization()(X)\n",
    "        return model.fit(X,y,**kwargs) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouver les hyper paramètres ainsi que s'il faut normaliser ou pas avec hyperBand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperband_tuner = kt.Hyperband(\n",
    "    MyClassificationHypermodel(),\n",
    "    objective=\"val_accuracy\",\n",
    "    factor=3, # commence à entrainer plein de modèles puis garde 1/factor des meilleurs et ainsi de suite jusqu'à ce qu'il n'en reste plus qu'un\n",
    "    hyperband_iterations=2, #Réalise l'opération entière 2 fois\n",
    "    max_epochs=8, \n",
    "    overwrite = True,\n",
    "    directory = \"my_fashion_mnist\",\n",
    "    project_name = \"hyperband\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 51s]\n",
      "val_accuracy: 0.8427500128746033\n",
      "\n",
      "Best val_accuracy So Far: 0.8575000166893005\n",
      "Total elapsed time: 00h 12m 01s\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(root_logdir)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=2)\n",
    "hyperband_tuner.search(X_train,y_train,epochs = 10,\n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [tensorboard_cb,early_stopping_cb])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une meillure solution est d'utiliser le BayesianOptimiser qui cherche la meilleure combianaiseon possible de params, mais pas de façon aléatoire. Il privilégie les hyperparamètres qui semblent les plus prometteurs au cours de l'exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha est le niveau de bruit que l'on souhaite dans la mesure des preformances entre essai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta est l'importance de l'exploration du tuner vis à vis de l'xploitation des bonnes régions connues de l'epace des hyperpaprams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuner = kt.BayesianOptimization(\n",
    "    MyClassificationHypermodel(),\n",
    "    objective='val_accuracy',\n",
    "    seed = 42,\n",
    "    max_trials = 10, # nombre de fois qu'il essai un nouveau model\n",
    "    alpha = 1e-4,\n",
    "    beta=2.6,\n",
    "    overwrite = True,\n",
    "    directory = \"my_fashion_mnist\",\n",
    "    project_name = \"bayesian\"\n",
    ")\n",
    "\n",
    "# bayesian_tuner.search([...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
